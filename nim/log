Playing training game 1
Choose action
[1, 3, 5, 7]
{}
[(0, 1), (2, 4), (1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (2, 3), (3, 3), (3, 6), (2, 2), (3, 2), (2, 5), (1, 3), (3, 5)]
(0, 1)
Choose action
[0, 3, 5, 7]
{}
[(2, 4), (1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (2, 3), (3, 3), (3, 6), (2, 2), (3, 2), (2, 5), (1, 3), (3, 5)]
(2, 4)
continue
Update state reward 0
[1, 3, 5, 7]
(0, 1)
Find best future reward
[0, 3, 1, 7]
Update q val
[1, 3, 5, 7]
(0, 1)
 new Q value 0.0
Choose action
[0, 3, 1, 7]
{((1, 3, 5, 7), (0, 1)): 0.0}
[(1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (1, 3), (3, 5)]
(1, 2)
continue
Update state reward 0
[0, 3, 5, 7]
(2, 4)
Find best future reward
[0, 1, 1, 7]
Update q val
[0, 3, 5, 7]
(2, 4)
 new Q value 0.0
Choose action
[0, 1, 1, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0}
[(2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (3, 5)]
(2, 1)
continue
Update state reward 0
[0, 3, 1, 7]
(1, 2)
Find best future reward
[0, 1, 0, 7]
Update q val
[0, 3, 1, 7]
(1, 2)
 new Q value 0.0
Choose action
[0, 1, 0, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0}
[(3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (3, 5)]
(3, 4)
continue
Update state reward 0
[0, 1, 1, 7]
(2, 1)
Find best future reward
[0, 1, 0, 3]
Update q val
[0, 1, 1, 7]
(2, 1)
 new Q value 0.0
Choose action
[0, 1, 0, 3]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.0}
[(3, 1), (1, 1), (3, 3), (3, 2)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 7]
(3, 4)
Find best future reward
[0, 1, 0, 2]
Update q val
[0, 1, 0, 7]
(3, 4)
 new Q value 0.0
Choose action
[0, 1, 0, 2]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.0, ((0, 1, 0, 7), (3, 4)): 0.0}
[(3, 1), (1, 1), (3, 2)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 3]
(3, 1)
Find best future reward
[0, 1, 0, 1]
Update q val
[0, 1, 0, 3]
(3, 1)
 new Q value 0.0
Choose action
[0, 1, 0, 1]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.0, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.0}
[(3, 1), (1, 1)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 2]
(3, 1)
Find best future reward
[0, 1, 0, 0]
Update q val
[0, 1, 0, 2]
(3, 1)
 new Q value 0.0
Choose action
[0, 1, 0, 0]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.0, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.0, ((0, 1, 0, 2), (3, 1)): 0.0}
[(1, 1)]
(1, 1)
Done
Update state reward -1
[0, 1, 0, 0]
(1, 1)
Find best future reward
[0, 0, 0, 0]
Update q val
[0, 1, 0, 0]
(1, 1)
 new Q value -0.5
Update state reward 1
[0, 1, 0, 1]
(3, 1)
Find best future reward
[0, 0, 0, 0]
Update q val
[0, 1, 0, 1]
(3, 1)
 new Q value 0.5
Playing training game 2
Choose action
[1, 3, 5, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.0, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.0, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.5, ((0, 1, 0, 1), (3, 1)): 0.5}
[(0, 1), (2, 4), (1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (2, 3), (3, 3), (3, 6), (2, 2), (3, 2), (2, 5), (1, 3), (3, 5)]
(0, 1)
Choose action
[0, 3, 5, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.0, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.0, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.5, ((0, 1, 0, 1), (3, 1)): 0.5}
[(2, 4), (1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (2, 3), (3, 3), (3, 6), (2, 2), (3, 2), (2, 5), (1, 3), (3, 5)]
(2, 4)
continue
Update state reward 0
[1, 3, 5, 7]
(0, 1)
Find best future reward
[0, 3, 1, 7]
Update q val
[1, 3, 5, 7]
(0, 1)
 new Q value 0.0
Choose action
[0, 3, 1, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.0, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.0, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.5, ((0, 1, 0, 1), (3, 1)): 0.5}
[(1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (1, 3), (3, 5)]
(1, 2)
continue
Update state reward 0
[0, 3, 5, 7]
(2, 4)
Find best future reward
[0, 1, 1, 7]
Update q val
[0, 3, 5, 7]
(2, 4)
 new Q value 0.0
Choose action
[0, 1, 1, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.0, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.0, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.5, ((0, 1, 0, 1), (3, 1)): 0.5}
[(2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (3, 5)]
(2, 1)
continue
Update state reward 0
[0, 3, 1, 7]
(1, 2)
Find best future reward
[0, 1, 0, 7]
Update q val
[0, 3, 1, 7]
(1, 2)
 new Q value 0.0
Choose action
[0, 1, 0, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.0, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.0, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.5, ((0, 1, 0, 1), (3, 1)): 0.5}
[(3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (3, 5)]
(3, 4)
continue
Update state reward 0
[0, 1, 1, 7]
(2, 1)
Find best future reward
[0, 1, 0, 3]
Update q val
[0, 1, 1, 7]
(2, 1)
 new Q value 0.0
Choose action
[0, 1, 0, 3]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.0, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.0, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.5, ((0, 1, 0, 1), (3, 1)): 0.5}
[(3, 1), (1, 1), (3, 3), (3, 2)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 7]
(3, 4)
Find best future reward
[0, 1, 0, 2]
Update q val
[0, 1, 0, 7]
(3, 4)
 new Q value 0.0
Choose action
[0, 1, 0, 2]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.0, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.0, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.5, ((0, 1, 0, 1), (3, 1)): 0.5}
[(3, 1), (1, 1), (3, 2)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 3]
(3, 1)
Find best future reward
[0, 1, 0, 1]
Update q val
[0, 1, 0, 3]
(3, 1)
 new Q value 0.25
Choose action
[0, 1, 0, 1]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.0, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.25, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.5, ((0, 1, 0, 1), (3, 1)): 0.5}
[(3, 1), (1, 1)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 2]
(3, 1)
Find best future reward
[0, 1, 0, 0]
Update q val
[0, 1, 0, 2]
(3, 1)
 new Q value 0.0
Choose action
[0, 1, 0, 0]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.0, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.25, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.5, ((0, 1, 0, 1), (3, 1)): 0.5}
[(1, 1)]
(1, 1)
Done
Update state reward -1
[0, 1, 0, 0]
(1, 1)
Find best future reward
[0, 0, 0, 0]
Update q val
[0, 1, 0, 0]
(1, 1)
 new Q value -0.75
Update state reward 1
[0, 1, 0, 1]
(3, 1)
Find best future reward
[0, 0, 0, 0]
Update q val
[0, 1, 0, 1]
(3, 1)
 new Q value 0.75
Playing training game 3
Choose action
[1, 3, 5, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.0, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.25, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.75, ((0, 1, 0, 1), (3, 1)): 0.75}
[(0, 1), (2, 4), (1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (2, 3), (3, 3), (3, 6), (2, 2), (3, 2), (2, 5), (1, 3), (3, 5)]
(0, 1)
Choose action
[0, 3, 5, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.0, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.25, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.75, ((0, 1, 0, 1), (3, 1)): 0.75}
[(2, 4), (1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (2, 3), (3, 3), (3, 6), (2, 2), (3, 2), (2, 5), (1, 3), (3, 5)]
(2, 4)
continue
Update state reward 0
[1, 3, 5, 7]
(0, 1)
Find best future reward
[0, 3, 1, 7]
Update q val
[1, 3, 5, 7]
(0, 1)
 new Q value 0.0
Choose action
[0, 3, 1, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.0, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.25, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.75, ((0, 1, 0, 1), (3, 1)): 0.75}
[(1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (1, 3), (3, 5)]
(1, 2)
continue
Update state reward 0
[0, 3, 5, 7]
(2, 4)
Find best future reward
[0, 1, 1, 7]
Update q val
[0, 3, 5, 7]
(2, 4)
 new Q value 0.0
Choose action
[0, 1, 1, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.0, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.25, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.75, ((0, 1, 0, 1), (3, 1)): 0.75}
[(2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (3, 5)]
(2, 1)
continue
Update state reward 0
[0, 3, 1, 7]
(1, 2)
Find best future reward
[0, 1, 0, 7]
Update q val
[0, 3, 1, 7]
(1, 2)
 new Q value 0.0
Choose action
[0, 1, 0, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.0, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.25, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.75, ((0, 1, 0, 1), (3, 1)): 0.75}
[(3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (3, 5)]
(3, 4)
continue
Update state reward 0
[0, 1, 1, 7]
(2, 1)
Find best future reward
[0, 1, 0, 3]
Update q val
[0, 1, 1, 7]
(2, 1)
 new Q value 0.125
Choose action
[0, 1, 0, 3]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.125, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.25, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.75, ((0, 1, 0, 1), (3, 1)): 0.75}
[(3, 1), (1, 1), (3, 3), (3, 2)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 7]
(3, 4)
Find best future reward
[0, 1, 0, 2]
Update q val
[0, 1, 0, 7]
(3, 4)
 new Q value 0.0
Choose action
[0, 1, 0, 2]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.125, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.25, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.75, ((0, 1, 0, 1), (3, 1)): 0.75}
[(3, 1), (1, 1), (3, 2)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 3]
(3, 1)
Find best future reward
[0, 1, 0, 1]
Update q val
[0, 1, 0, 3]
(3, 1)
 new Q value 0.5
Choose action
[0, 1, 0, 1]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.125, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.5, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.75, ((0, 1, 0, 1), (3, 1)): 0.75}
[(3, 1), (1, 1)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 2]
(3, 1)
Find best future reward
[0, 1, 0, 0]
Update q val
[0, 1, 0, 2]
(3, 1)
 new Q value 0.0
Choose action
[0, 1, 0, 0]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.125, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.5, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.75, ((0, 1, 0, 1), (3, 1)): 0.75}
[(1, 1)]
(1, 1)
Done
Update state reward -1
[0, 1, 0, 0]
(1, 1)
Find best future reward
[0, 0, 0, 0]
Update q val
[0, 1, 0, 0]
(1, 1)
 new Q value -0.875
Update state reward 1
[0, 1, 0, 1]
(3, 1)
Find best future reward
[0, 0, 0, 0]
Update q val
[0, 1, 0, 1]
(3, 1)
 new Q value 0.875
Playing training game 4
Choose action
[1, 3, 5, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.125, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.5, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.875, ((0, 1, 0, 1), (3, 1)): 0.875}
[(0, 1), (2, 4), (1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (2, 3), (3, 3), (3, 6), (2, 2), (3, 2), (2, 5), (1, 3), (3, 5)]
(0, 1)
Choose action
[0, 3, 5, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.125, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.5, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.875, ((0, 1, 0, 1), (3, 1)): 0.875}
[(2, 4), (1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (2, 3), (3, 3), (3, 6), (2, 2), (3, 2), (2, 5), (1, 3), (3, 5)]
(2, 4)
continue
Update state reward 0
[1, 3, 5, 7]
(0, 1)
Find best future reward
[0, 3, 1, 7]
Update q val
[1, 3, 5, 7]
(0, 1)
 new Q value 0.0
Choose action
[0, 3, 1, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.125, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.5, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.875, ((0, 1, 0, 1), (3, 1)): 0.875}
[(1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (1, 3), (3, 5)]
(1, 2)
continue
Update state reward 0
[0, 3, 5, 7]
(2, 4)
Find best future reward
[0, 1, 1, 7]
Update q val
[0, 3, 5, 7]
(2, 4)
 new Q value 0.0625
Choose action
[0, 1, 1, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0625, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.125, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.5, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.875, ((0, 1, 0, 1), (3, 1)): 0.875}
[(2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (3, 5)]
(2, 1)
continue
Update state reward 0
[0, 3, 1, 7]
(1, 2)
Find best future reward
[0, 1, 0, 7]
Update q val
[0, 3, 1, 7]
(1, 2)
 new Q value 0.0
Choose action
[0, 1, 0, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0625, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.125, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.5, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.875, ((0, 1, 0, 1), (3, 1)): 0.875}
[(3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (3, 5)]
(3, 4)
continue
Update state reward 0
[0, 1, 1, 7]
(2, 1)
Find best future reward
[0, 1, 0, 3]
Update q val
[0, 1, 1, 7]
(2, 1)
 new Q value 0.3125
Choose action
[0, 1, 0, 3]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0625, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.3125, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.5, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.875, ((0, 1, 0, 1), (3, 1)): 0.875}
[(3, 1), (1, 1), (3, 3), (3, 2)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 7]
(3, 4)
Find best future reward
[0, 1, 0, 2]
Update q val
[0, 1, 0, 7]
(3, 4)
 new Q value 0.0
Choose action
[0, 1, 0, 2]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0625, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.3125, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.5, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.875, ((0, 1, 0, 1), (3, 1)): 0.875}
[(3, 1), (1, 1), (3, 2)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 3]
(3, 1)
Find best future reward
[0, 1, 0, 1]
Update q val
[0, 1, 0, 3]
(3, 1)
 new Q value 0.6875
Choose action
[0, 1, 0, 1]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0625, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.3125, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.6875, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.875, ((0, 1, 0, 1), (3, 1)): 0.875}
[(3, 1), (1, 1)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 2]
(3, 1)
Find best future reward
[0, 1, 0, 0]
Update q val
[0, 1, 0, 2]
(3, 1)
 new Q value 0.0
Choose action
[0, 1, 0, 0]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0625, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.3125, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.6875, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.875, ((0, 1, 0, 1), (3, 1)): 0.875}
[(1, 1)]
(1, 1)
Done
Update state reward -1
[0, 1, 0, 0]
(1, 1)
Find best future reward
[0, 0, 0, 0]
Update q val
[0, 1, 0, 0]
(1, 1)
 new Q value -0.9375
Update state reward 1
[0, 1, 0, 1]
(3, 1)
Find best future reward
[0, 0, 0, 0]
Update q val
[0, 1, 0, 1]
(3, 1)
 new Q value 0.9375
Playing training game 5
Choose action
[1, 3, 5, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0625, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.3125, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.6875, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.9375, ((0, 1, 0, 1), (3, 1)): 0.9375}
[(0, 1), (2, 4), (1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (2, 3), (3, 3), (3, 6), (2, 2), (3, 2), (2, 5), (1, 3), (3, 5)]
(0, 1)
Choose action
[0, 3, 5, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0625, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.3125, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.6875, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.9375, ((0, 1, 0, 1), (3, 1)): 0.9375}
[(2, 4), (1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (2, 3), (3, 3), (3, 6), (2, 2), (3, 2), (2, 5), (1, 3), (3, 5)]
(2, 4)
continue
Update state reward 0
[1, 3, 5, 7]
(0, 1)
Find best future reward
[0, 3, 1, 7]
Update q val
[1, 3, 5, 7]
(0, 1)
 new Q value 0.0
Choose action
[0, 3, 1, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.0625, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.3125, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.6875, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.9375, ((0, 1, 0, 1), (3, 1)): 0.9375}
[(1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (1, 3), (3, 5)]
(1, 2)
continue
Update state reward 0
[0, 3, 5, 7]
(2, 4)
Find best future reward
[0, 1, 1, 7]
Update q val
[0, 3, 5, 7]
(2, 4)
 new Q value 0.1875
Choose action
[0, 1, 1, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.1875, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.3125, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.6875, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.9375, ((0, 1, 0, 1), (3, 1)): 0.9375}
[(2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (3, 5)]
(2, 1)
continue
Update state reward 0
[0, 3, 1, 7]
(1, 2)
Find best future reward
[0, 1, 0, 7]
Update q val
[0, 3, 1, 7]
(1, 2)
 new Q value 0.0
Choose action
[0, 1, 0, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.1875, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.3125, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.6875, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.9375, ((0, 1, 0, 1), (3, 1)): 0.9375}
[(3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (3, 5)]
(3, 4)
continue
Update state reward 0
[0, 1, 1, 7]
(2, 1)
Find best future reward
[0, 1, 0, 3]
Update q val
[0, 1, 1, 7]
(2, 1)
 new Q value 0.5
Choose action
[0, 1, 0, 3]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.1875, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.5, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.6875, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.9375, ((0, 1, 0, 1), (3, 1)): 0.9375}
[(3, 1), (1, 1), (3, 3), (3, 2)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 7]
(3, 4)
Find best future reward
[0, 1, 0, 2]
Update q val
[0, 1, 0, 7]
(3, 4)
 new Q value 0.0
Choose action
[0, 1, 0, 2]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.1875, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.5, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.6875, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.9375, ((0, 1, 0, 1), (3, 1)): 0.9375}
[(3, 1), (1, 1), (3, 2)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 3]
(3, 1)
Find best future reward
[0, 1, 0, 1]
Update q val
[0, 1, 0, 3]
(3, 1)
 new Q value 0.8125
Choose action
[0, 1, 0, 1]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.1875, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.5, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.8125, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.9375, ((0, 1, 0, 1), (3, 1)): 0.9375}
[(3, 1), (1, 1)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 2]
(3, 1)
Find best future reward
[0, 1, 0, 0]
Update q val
[0, 1, 0, 2]
(3, 1)
 new Q value 0.0
Choose action
[0, 1, 0, 0]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.1875, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.5, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.8125, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.9375, ((0, 1, 0, 1), (3, 1)): 0.9375}
[(1, 1)]
(1, 1)
Done
Update state reward -1
[0, 1, 0, 0]
(1, 1)
Find best future reward
[0, 0, 0, 0]
Update q val
[0, 1, 0, 0]
(1, 1)
 new Q value -0.96875
Update state reward 1
[0, 1, 0, 1]
(3, 1)
Find best future reward
[0, 0, 0, 0]
Update q val
[0, 1, 0, 1]
(3, 1)
 new Q value 0.96875
Playing training game 6
Choose action
[1, 3, 5, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.1875, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.5, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.8125, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.96875, ((0, 1, 0, 1), (3, 1)): 0.96875}
[(0, 1), (2, 4), (1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (2, 3), (3, 3), (3, 6), (2, 2), (3, 2), (2, 5), (1, 3), (3, 5)]
(0, 1)
Choose action
[0, 3, 5, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.1875, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.5, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.8125, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.96875, ((0, 1, 0, 1), (3, 1)): 0.96875}
[(2, 4), (1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (2, 3), (3, 3), (3, 6), (2, 2), (3, 2), (2, 5), (1, 3), (3, 5)]
(2, 4)
continue
Update state reward 0
[1, 3, 5, 7]
(0, 1)
Find best future reward
[0, 3, 1, 7]
Update q val
[1, 3, 5, 7]
(0, 1)
 new Q value 0.0
Choose action
[0, 3, 1, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.1875, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.5, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.8125, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.96875, ((0, 1, 0, 1), (3, 1)): 0.96875}
[(1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (1, 3), (3, 5)]
(1, 2)
continue
Update state reward 0
[0, 3, 5, 7]
(2, 4)
Find best future reward
[0, 1, 1, 7]
Update q val
[0, 3, 5, 7]
(2, 4)
 new Q value 0.34375
Choose action
[0, 1, 1, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.34375, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.5, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.8125, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.96875, ((0, 1, 0, 1), (3, 1)): 0.96875}
[(2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (3, 5)]
(2, 1)
continue
Update state reward 0
[0, 3, 1, 7]
(1, 2)
Find best future reward
[0, 1, 0, 7]
Update q val
[0, 3, 1, 7]
(1, 2)
 new Q value 0.0
Choose action
[0, 1, 0, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.34375, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.5, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.8125, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.96875, ((0, 1, 0, 1), (3, 1)): 0.96875}
[(3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (3, 5)]
(3, 4)
continue
Update state reward 0
[0, 1, 1, 7]
(2, 1)
Find best future reward
[0, 1, 0, 3]
Update q val
[0, 1, 1, 7]
(2, 1)
 new Q value 0.65625
Choose action
[0, 1, 0, 3]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.34375, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.65625, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.8125, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.96875, ((0, 1, 0, 1), (3, 1)): 0.96875}
[(3, 1), (1, 1), (3, 3), (3, 2)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 7]
(3, 4)
Find best future reward
[0, 1, 0, 2]
Update q val
[0, 1, 0, 7]
(3, 4)
 new Q value 0.0
Choose action
[0, 1, 0, 2]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.34375, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.65625, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.8125, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.96875, ((0, 1, 0, 1), (3, 1)): 0.96875}
[(3, 1), (1, 1), (3, 2)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 3]
(3, 1)
Find best future reward
[0, 1, 0, 1]
Update q val
[0, 1, 0, 3]
(3, 1)
 new Q value 0.890625
Choose action
[0, 1, 0, 1]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.34375, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.65625, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.890625, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.96875, ((0, 1, 0, 1), (3, 1)): 0.96875}
[(3, 1), (1, 1)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 2]
(3, 1)
Find best future reward
[0, 1, 0, 0]
Update q val
[0, 1, 0, 2]
(3, 1)
 new Q value 0.0
Choose action
[0, 1, 0, 0]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.34375, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.65625, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.890625, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.96875, ((0, 1, 0, 1), (3, 1)): 0.96875}
[(1, 1)]
(1, 1)
Done
Update state reward -1
[0, 1, 0, 0]
(1, 1)
Find best future reward
[0, 0, 0, 0]
Update q val
[0, 1, 0, 0]
(1, 1)
 new Q value -0.984375
Update state reward 1
[0, 1, 0, 1]
(3, 1)
Find best future reward
[0, 0, 0, 0]
Update q val
[0, 1, 0, 1]
(3, 1)
 new Q value 0.984375
Playing training game 7
Choose action
[1, 3, 5, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.34375, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.65625, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.890625, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.984375, ((0, 1, 0, 1), (3, 1)): 0.984375}
[(0, 1), (2, 4), (1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (2, 3), (3, 3), (3, 6), (2, 2), (3, 2), (2, 5), (1, 3), (3, 5)]
(0, 1)
Choose action
[0, 3, 5, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.34375, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.65625, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.890625, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.984375, ((0, 1, 0, 1), (3, 1)): 0.984375}
[(2, 4), (1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (2, 3), (3, 3), (3, 6), (2, 2), (3, 2), (2, 5), (1, 3), (3, 5)]
(2, 4)
continue
Update state reward 0
[1, 3, 5, 7]
(0, 1)
Find best future reward
[0, 3, 1, 7]
Update q val
[1, 3, 5, 7]
(0, 1)
 new Q value 0.0
Choose action
[0, 3, 1, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.34375, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.65625, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.890625, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.984375, ((0, 1, 0, 1), (3, 1)): 0.984375}
[(1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (1, 3), (3, 5)]
(1, 2)
continue
Update state reward 0
[0, 3, 5, 7]
(2, 4)
Find best future reward
[0, 1, 1, 7]
Update q val
[0, 3, 5, 7]
(2, 4)
 new Q value 0.5
Choose action
[0, 1, 1, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.5, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.65625, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.890625, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.984375, ((0, 1, 0, 1), (3, 1)): 0.984375}
[(2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (3, 5)]
(2, 1)
continue
Update state reward 0
[0, 3, 1, 7]
(1, 2)
Find best future reward
[0, 1, 0, 7]
Update q val
[0, 3, 1, 7]
(1, 2)
 new Q value 0.0
Choose action
[0, 1, 0, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.5, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.65625, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.890625, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.984375, ((0, 1, 0, 1), (3, 1)): 0.984375}
[(3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (3, 5)]
(3, 4)
continue
Update state reward 0
[0, 1, 1, 7]
(2, 1)
Find best future reward
[0, 1, 0, 3]
Update q val
[0, 1, 1, 7]
(2, 1)
 new Q value 0.7734375
Choose action
[0, 1, 0, 3]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.5, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.7734375, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.890625, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.984375, ((0, 1, 0, 1), (3, 1)): 0.984375}
[(3, 1), (1, 1), (3, 3), (3, 2)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 7]
(3, 4)
Find best future reward
[0, 1, 0, 2]
Update q val
[0, 1, 0, 7]
(3, 4)
 new Q value 0.0
Choose action
[0, 1, 0, 2]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.5, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.7734375, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.890625, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.984375, ((0, 1, 0, 1), (3, 1)): 0.984375}
[(3, 1), (1, 1), (3, 2)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 3]
(3, 1)
Find best future reward
[0, 1, 0, 1]
Update q val
[0, 1, 0, 3]
(3, 1)
 new Q value 0.9375
Choose action
[0, 1, 0, 1]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.5, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.7734375, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.9375, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.984375, ((0, 1, 0, 1), (3, 1)): 0.984375}
[(3, 1), (1, 1)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 2]
(3, 1)
Find best future reward
[0, 1, 0, 0]
Update q val
[0, 1, 0, 2]
(3, 1)
 new Q value 0.0
Choose action
[0, 1, 0, 0]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.5, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.7734375, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.9375, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.984375, ((0, 1, 0, 1), (3, 1)): 0.984375}
[(1, 1)]
(1, 1)
Done
Update state reward -1
[0, 1, 0, 0]
(1, 1)
Find best future reward
[0, 0, 0, 0]
Update q val
[0, 1, 0, 0]
(1, 1)
 new Q value -0.9921875
Update state reward 1
[0, 1, 0, 1]
(3, 1)
Find best future reward
[0, 0, 0, 0]
Update q val
[0, 1, 0, 1]
(3, 1)
 new Q value 0.9921875
Playing training game 8
Choose action
[1, 3, 5, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.5, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.7734375, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.9375, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.9921875, ((0, 1, 0, 1), (3, 1)): 0.9921875}
[(0, 1), (2, 4), (1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (2, 3), (3, 3), (3, 6), (2, 2), (3, 2), (2, 5), (1, 3), (3, 5)]
(0, 1)
Choose action
[0, 3, 5, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.5, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.7734375, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.9375, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.9921875, ((0, 1, 0, 1), (3, 1)): 0.9921875}
[(2, 4), (1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (2, 3), (3, 3), (3, 6), (2, 2), (3, 2), (2, 5), (1, 3), (3, 5)]
(2, 4)
continue
Update state reward 0
[1, 3, 5, 7]
(0, 1)
Find best future reward
[0, 3, 1, 7]
Update q val
[1, 3, 5, 7]
(0, 1)
 new Q value 0.0
Choose action
[0, 3, 1, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.5, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.7734375, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.9375, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.9921875, ((0, 1, 0, 1), (3, 1)): 0.9921875}
[(1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (1, 3), (3, 5)]
(1, 2)
continue
Update state reward 0
[0, 3, 5, 7]
(2, 4)
Find best future reward
[0, 1, 1, 7]
Update q val
[0, 3, 5, 7]
(2, 4)
 new Q value 0.63671875
Choose action
[0, 1, 1, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.63671875, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.7734375, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.9375, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.9921875, ((0, 1, 0, 1), (3, 1)): 0.9921875}
[(2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (3, 5)]
(2, 1)
continue
Update state reward 0
[0, 3, 1, 7]
(1, 2)
Find best future reward
[0, 1, 0, 7]
Update q val
[0, 3, 1, 7]
(1, 2)
 new Q value 0.0
Choose action
[0, 1, 0, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.63671875, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.7734375, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.9375, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.9921875, ((0, 1, 0, 1), (3, 1)): 0.9921875}
[(3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (3, 5)]
(3, 4)
continue
Update state reward 0
[0, 1, 1, 7]
(2, 1)
Find best future reward
[0, 1, 0, 3]
Update q val
[0, 1, 1, 7]
(2, 1)
 new Q value 0.85546875
Choose action
[0, 1, 0, 3]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.63671875, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.85546875, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.9375, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.9921875, ((0, 1, 0, 1), (3, 1)): 0.9921875}
[(3, 1), (1, 1), (3, 3), (3, 2)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 7]
(3, 4)
Find best future reward
[0, 1, 0, 2]
Update q val
[0, 1, 0, 7]
(3, 4)
 new Q value 0.0
Choose action
[0, 1, 0, 2]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.63671875, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.85546875, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.9375, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.9921875, ((0, 1, 0, 1), (3, 1)): 0.9921875}
[(3, 1), (1, 1), (3, 2)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 3]
(3, 1)
Find best future reward
[0, 1, 0, 1]
Update q val
[0, 1, 0, 3]
(3, 1)
 new Q value 0.96484375
Choose action
[0, 1, 0, 1]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.63671875, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.85546875, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.96484375, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.9921875, ((0, 1, 0, 1), (3, 1)): 0.9921875}
[(3, 1), (1, 1)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 2]
(3, 1)
Find best future reward
[0, 1, 0, 0]
Update q val
[0, 1, 0, 2]
(3, 1)
 new Q value 0.0
Choose action
[0, 1, 0, 0]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.63671875, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.85546875, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.96484375, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.9921875, ((0, 1, 0, 1), (3, 1)): 0.9921875}
[(1, 1)]
(1, 1)
Done
Update state reward -1
[0, 1, 0, 0]
(1, 1)
Find best future reward
[0, 0, 0, 0]
Update q val
[0, 1, 0, 0]
(1, 1)
 new Q value -0.99609375
Update state reward 1
[0, 1, 0, 1]
(3, 1)
Find best future reward
[0, 0, 0, 0]
Update q val
[0, 1, 0, 1]
(3, 1)
 new Q value 0.99609375
Playing training game 9
Choose action
[1, 3, 5, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.63671875, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.85546875, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.96484375, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.99609375, ((0, 1, 0, 1), (3, 1)): 0.99609375}
[(0, 1), (2, 4), (1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (2, 3), (3, 3), (3, 6), (2, 2), (3, 2), (2, 5), (1, 3), (3, 5)]
(0, 1)
Choose action
[0, 3, 5, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.63671875, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.85546875, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.96484375, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.99609375, ((0, 1, 0, 1), (3, 1)): 0.99609375}
[(2, 4), (1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (2, 3), (3, 3), (3, 6), (2, 2), (3, 2), (2, 5), (1, 3), (3, 5)]
(2, 4)
continue
Update state reward 0
[1, 3, 5, 7]
(0, 1)
Find best future reward
[0, 3, 1, 7]
Update q val
[1, 3, 5, 7]
(0, 1)
 new Q value 0.0
Choose action
[0, 3, 1, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.63671875, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.85546875, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.96484375, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.99609375, ((0, 1, 0, 1), (3, 1)): 0.99609375}
[(1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (1, 3), (3, 5)]
(1, 2)
continue
Update state reward 0
[0, 3, 5, 7]
(2, 4)
Find best future reward
[0, 1, 1, 7]
Update q val
[0, 3, 5, 7]
(2, 4)
 new Q value 0.74609375
Choose action
[0, 1, 1, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.74609375, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.85546875, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.96484375, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.99609375, ((0, 1, 0, 1), (3, 1)): 0.99609375}
[(2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (3, 5)]
(2, 1)
continue
Update state reward 0
[0, 3, 1, 7]
(1, 2)
Find best future reward
[0, 1, 0, 7]
Update q val
[0, 3, 1, 7]
(1, 2)
 new Q value 0.0
Choose action
[0, 1, 0, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.74609375, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.85546875, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.96484375, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.99609375, ((0, 1, 0, 1), (3, 1)): 0.99609375}
[(3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (3, 5)]
(3, 4)
continue
Update state reward 0
[0, 1, 1, 7]
(2, 1)
Find best future reward
[0, 1, 0, 3]
Update q val
[0, 1, 1, 7]
(2, 1)
 new Q value 0.91015625
Choose action
[0, 1, 0, 3]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.74609375, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.91015625, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.96484375, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.99609375, ((0, 1, 0, 1), (3, 1)): 0.99609375}
[(3, 1), (1, 1), (3, 3), (3, 2)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 7]
(3, 4)
Find best future reward
[0, 1, 0, 2]
Update q val
[0, 1, 0, 7]
(3, 4)
 new Q value 0.0
Choose action
[0, 1, 0, 2]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.74609375, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.91015625, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.96484375, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.99609375, ((0, 1, 0, 1), (3, 1)): 0.99609375}
[(3, 1), (1, 1), (3, 2)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 3]
(3, 1)
Find best future reward
[0, 1, 0, 1]
Update q val
[0, 1, 0, 3]
(3, 1)
 new Q value 0.98046875
Choose action
[0, 1, 0, 1]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.74609375, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.91015625, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.98046875, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.99609375, ((0, 1, 0, 1), (3, 1)): 0.99609375}
[(3, 1), (1, 1)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 2]
(3, 1)
Find best future reward
[0, 1, 0, 0]
Update q val
[0, 1, 0, 2]
(3, 1)
 new Q value 0.0
Choose action
[0, 1, 0, 0]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.74609375, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.91015625, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.98046875, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.99609375, ((0, 1, 0, 1), (3, 1)): 0.99609375}
[(1, 1)]
(1, 1)
Done
Update state reward -1
[0, 1, 0, 0]
(1, 1)
Find best future reward
[0, 0, 0, 0]
Update q val
[0, 1, 0, 0]
(1, 1)
 new Q value -0.998046875
Update state reward 1
[0, 1, 0, 1]
(3, 1)
Find best future reward
[0, 0, 0, 0]
Update q val
[0, 1, 0, 1]
(3, 1)
 new Q value 0.998046875
Playing training game 10
Choose action
[1, 3, 5, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.74609375, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.91015625, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.98046875, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.998046875, ((0, 1, 0, 1), (3, 1)): 0.998046875}
[(0, 1), (2, 4), (1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (2, 3), (3, 3), (3, 6), (2, 2), (3, 2), (2, 5), (1, 3), (3, 5)]
(0, 1)
Choose action
[0, 3, 5, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.74609375, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.91015625, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.98046875, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.998046875, ((0, 1, 0, 1), (3, 1)): 0.998046875}
[(2, 4), (1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (2, 3), (3, 3), (3, 6), (2, 2), (3, 2), (2, 5), (1, 3), (3, 5)]
(2, 4)
continue
Update state reward 0
[1, 3, 5, 7]
(0, 1)
Find best future reward
[0, 3, 1, 7]
Update q val
[1, 3, 5, 7]
(0, 1)
 new Q value 0.0
Choose action
[0, 3, 1, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.74609375, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.91015625, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.98046875, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.998046875, ((0, 1, 0, 1), (3, 1)): 0.998046875}
[(1, 2), (2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (1, 3), (3, 5)]
(1, 2)
continue
Update state reward 0
[0, 3, 5, 7]
(2, 4)
Find best future reward
[0, 1, 1, 7]
Update q val
[0, 3, 5, 7]
(2, 4)
 new Q value 0.828125
Choose action
[0, 1, 1, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.828125, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.91015625, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.98046875, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.998046875, ((0, 1, 0, 1), (3, 1)): 0.998046875}
[(2, 1), (3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (3, 5)]
(2, 1)
continue
Update state reward 0
[0, 3, 1, 7]
(1, 2)
Find best future reward
[0, 1, 0, 7]
Update q val
[0, 3, 1, 7]
(1, 2)
 new Q value 0.0
Choose action
[0, 1, 0, 7]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.828125, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.91015625, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.98046875, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.998046875, ((0, 1, 0, 1), (3, 1)): 0.998046875}
[(3, 4), (3, 1), (3, 7), (1, 1), (3, 3), (3, 6), (3, 2), (3, 5)]
(3, 4)
continue
Update state reward 0
[0, 1, 1, 7]
(2, 1)
Find best future reward
[0, 1, 0, 3]
Update q val
[0, 1, 1, 7]
(2, 1)
 new Q value 0.9453125
Choose action
[0, 1, 0, 3]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.828125, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.9453125, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.98046875, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.998046875, ((0, 1, 0, 1), (3, 1)): 0.998046875}
[(3, 1), (1, 1), (3, 3), (3, 2)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 7]
(3, 4)
Find best future reward
[0, 1, 0, 2]
Update q val
[0, 1, 0, 7]
(3, 4)
 new Q value 0.0
Choose action
[0, 1, 0, 2]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.828125, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.9453125, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.98046875, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.998046875, ((0, 1, 0, 1), (3, 1)): 0.998046875}
[(3, 1), (1, 1), (3, 2)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 3]
(3, 1)
Find best future reward
[0, 1, 0, 1]
Update q val
[0, 1, 0, 3]
(3, 1)
 new Q value 0.9892578125
Choose action
[0, 1, 0, 1]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.828125, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.9453125, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.9892578125, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.998046875, ((0, 1, 0, 1), (3, 1)): 0.998046875}
[(3, 1), (1, 1)]
(3, 1)
continue
Update state reward 0
[0, 1, 0, 2]
(3, 1)
Find best future reward
[0, 1, 0, 0]
Update q val
[0, 1, 0, 2]
(3, 1)
 new Q value 0.0
Choose action
[0, 1, 0, 0]
{((1, 3, 5, 7), (0, 1)): 0.0, ((0, 3, 5, 7), (2, 4)): 0.828125, ((0, 3, 1, 7), (1, 2)): 0.0, ((0, 1, 1, 7), (2, 1)): 0.9453125, ((0, 1, 0, 7), (3, 4)): 0.0, ((0, 1, 0, 3), (3, 1)): 0.9892578125, ((0, 1, 0, 2), (3, 1)): 0.0, ((0, 1, 0, 0), (1, 1)): -0.998046875, ((0, 1, 0, 1), (3, 1)): 0.998046875}
[(1, 1)]
(1, 1)
Done
Update state reward -1
[0, 1, 0, 0]
(1, 1)
Find best future reward
[0, 0, 0, 0]
Update q val
[0, 1, 0, 0]
(1, 1)
 new Q value -0.9990234375
Update state reward 1
[0, 1, 0, 1]
(3, 1)
Find best future reward
[0, 0, 0, 0]
Update q val
[0, 1, 0, 1]
(3, 1)
 new Q value 0.9990234375
Done training
